# 1.2 Datenvorverarbeitung

Die Datenvorverarbeitung ist ein wesentlicher Schritt im maschinellen Lernen, um Rohdaten in eine Form zu bringen, die für Modelle verarbeitbar sind. Dieser Prozess umfasst Schritte wie die Sammlung, Bereinigung, Exploration und Visualisierung von Daten, das Entfernen unerwünschter oder fehlerhafter Daten, das Feature Engineering und das Aufteilen der Daten in Trainings-, Validierungs- und Testdatensätze.

#### 1.2.1 **Datenbeschaffung**

Die Datensammlung beinhaltet das Beschaffen von Daten, die für das Machine-Learning-Projekt relevant sind. Diese Daten können aus verschiedenen Quellen stammen, darunter Sensoren (Daten, die selbst erfasst wurden), Online-Datenbanken oder APIs (Programmierschnittstellen, die den Zugriff auf bestimmte Daten oder Dienste ermöglichen). Im folgenden werden die Open-Source Datensätze vorgestellt, die in den Kapiteln Maschinelles Lernen und TinyML zur Veranschaulichung verwendet werden.

Ein Beispiel für einen Open-Source Datensatz ist der **Iris-Datensatzes**, welcher bereits in vielen Machine-Learning-Bibliotheken enthalten ist und leicht geladen werden kann. Der Iris-Datensatz (engl. für Schwertlilie) ist eine vordefinierte Sammlung von Messungen, die die Blütenmerkmale von drei verschiedenen Schwertlilien-Arten repräsentieren. Er wird häufig in Machine-Learning-Projekten verwendet, um Algorithmen für Klassifikations- und Mustererkennungsaufgaben zu trainieren und zu evaluieren.

Der **Digit-Datensatz** besteht aus handgeschriebenen Ziffern von Null bis Neun. Die Merkmale jedes Bildes können Pixelwerte, Konturen, geometrische Eigenschaften, Histogramme und Texturmerkmale umfassen. Diese Merkmale werden verwendet, um Algorithmen für die optische Zeichenerkennung zu trainieren und zu evaluieren

Der **Breast Cancer-Datensatz** umfasst klinische Messungen, die bei der Diagnose von Brustkrebs verwendet werden können. Er enthält Merkmale wie Tumorgröße, Knotentyp und andere Faktoren, die Ärzten helfen können, Tumoren als gutartig oder bösartig zu klassifizieren. Dieser Datensatz ist ein wichtiges Instrument für Machine-Learning-Anwendungen im medizinischen Bereich, insbesondere für die Klassifizierung von Tumoren und die Unterstützung von Diagnoseentscheidungen.

Der **MNIST-Datensatz** ist ein bekannter Benchmark für die Bilderkennung und besteht aus handgeschriebenen Ziffern von Null bis Neun, ähnlich dem Digit-Datensatz. Der Hauptunterschied zwischen dem MNIST- und dem Digit-Datensatz liegt in der Zusammensetzung der Daten. Während der Digit-Datensatz handgeschriebene Ziffern von Null bis Neun aller Art enthält, konzentriert sich der MNIST-Datensatz ausschließlich auf handgeschriebene Ziffern von Null bis Neun in einem standardisierten Format. Jede Instanz im MNIST-Datensatz ist ein 28x28 Pixel großes Graustufenbild, das eine einzelne handgeschriebene Ziffer darstellt. Die Merkmale dieser Bilder können ebenfalls Pixelwerte, Konturen, geometrische Eigenschaften, Histogramme und Texturmerkmale umfassen. Beide Datensätze dienen jedoch demselben Zweck und werden häufig für die Entwicklung und Evaluierung von Bilderkennungsalgorithmen verwendet.

[Quelle/weitere Infos](https://www.tensorflow.org/datasets), [Quelle/weitere Infos 2](https://pytorch.org/vision/stable/datasets.html), [Quelle/weitere Infos 3](https://scikit-learn.org/stable/datasets.html), [Quelle/weitere Infos 4](https://www.kaggle.com/datasets), [Quelle/weitere Infos 5](https://paperswithcode.com/datasets), [Quelle/weitere Infos 6](https://huggingface.co/datasets), [Quelle/weitere Infos 7](https://catalog.data.gov/dataset) [scikit-learn/The Digit Dataset](https://scikit-learn.org/stable/auto\_examples/datasets/plot\_digits\_last\_image.html) [scikit-learn/breast cancer](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load\_breast\_cancer.html) [TensorFlow/mnist](https://www.tensorflow.org/datasets/catalog/mnist)

#### 1.2.2 **Datenbereinigung (Data Cleaning)**

Daten sind oft unvollständig oder fehlerhaft. Dies ist vor allem der Fall wenn Daten verwendet werden, die selber erhoben wurden. Daten aus Online-Datenbanken oder die in APIs integriert sind wurden oftmals schon vorverarbeitet. In der Datenbereinigung werden die Probleme der unvollständigen und fehlerhaften Daten angegangen.

**1. Fehlende Werte behandeln:** Fehlende Werte bei Datensätzen können dazu führen, dass Machine Learning-Modelle sich während des Trainings nicht so gut an die Daten anpassen können. Bei späteren Vorhersagen des Modell führt dies dann zu fehlerhaften oder unvollständigen Vorhersagen. Die Bereinigung umfasst das Identifizieren fehlender Werte und deren Behandlung. Dies kann durch Auffüllen mit Durchschnittswerten, Löschen der betreffenden Stellen des Datensatzes oder andere Strategien erfolgen (Siehe Codebeispiel).

[Quelle/weitere Infos](https://www.datacamp.com/tutorial/techniques-to-handle-missing-data-values), [Quelle/weitere Infos 2](https://www.analyticsvidhya.com/blog/2021/10/handling-missing-value/), [Quelle/weitere Infos 3](https://www.kaggle.com/code/alexisbcook/missing-values), [Quelle/weitere Infos 4](http://pandas.pydata.org/pandas-docs/stable/user\_guide/missing\_data.html), [Quelle/weitere Infos 5](https://machinelearningmastery.com/handle-missing-data-python/)

**1. Duplikate entfernen:** Duplikate in den Daten können zu Verzerrungen führen, da sie denselben Datenpunkt mehrmals repräsentieren. Es ist wichtig, Duplikate zu identifizieren und zu entfernen, um sicherzustellen, dass jeder Datenpunkt nur einmal in der Analyse berücksichtigt wird (Siehe Codebeispiel).

[Quelle/weitere Infos](https://www.ritchieng.com/pandas-removing-duplicate-rows/), [Quelle/weitere Infos 2](https://medium.com/@ayushmandurgapal/handling-duplicate-values-and-outliers-in-a-dataset-b00ce130818e), [Quelle/weitere Infos 3](https://deepchecks.com/what-is-data-cleaning/), [Quelle/weitere Infos 4](https://medium.com/@anishnama20/how-duplicate-entries-in-data-set-leads-to-ovetfitting-2e3376e309c5)

**3. Ausreißer erkennen und behandeln:** Ausreißer sind ungewöhnliche Datenpunkte, die stark von den restlichen Daten abweichen. Sie entstehen oft durch Messfehler oder ungewöhnliche Ereignisse, die nicht repräsentativ für die normale Datenverteilung sind. Ausreißer können die Leistung von Machine Learning-Modellen beeinträchtigen. Die Bereinigung beinhaltet das Identifizieren und Behandeln von Ausreißern, entweder durch Entfernen oder Transformation. (Siehe Codebeispiel)

[Quelle/weitere Infos](https://www.analyticsvidhya.com/blog/2021/05/detecting-and-treating-outliers-treating-the-odd-one-out/), [Quelle/weitere Infos 2](https://medium.com/analytics-vidhya/how-to-remove-outliers-for-machine-learning-24620c4657e8), [Quelle/weitere Infos 3](https://towardsdatascience.com/outlier-detection-methods-in-machine-learning-1c8b7cca6cb8), [Quelle/weitere Infos 4](https://www.neuraldesigner.com/blog/effective-outlier-treatment-methods-machine-learning/)

**4. Konsistenz sicherstellen:** Die Daten sollten konsistent sein und denselben Formatierungsstandards folgen. Das bedeutet beispielsweise das Vereinheitlichen von Datentypen (z.B. Datumswerte), das Angleichen von Einheiten (z.B. Umrechnung von Längen) und z.B. die Standardisierung von Kategorienamen. Dadurch wird die Qualität der Daten verbessert und ihre Verwendbarkeit in Analysen und Machine Learning-Modellen gesteigert. (Siehe Codebeispiel)

[Quelle/weiter Infos](https://www.future-processing.com/blog/data-preprocessing-a-comprehensive-step-by-step-guide/#the-role-of-data-quality-in-preprocessing)

**5. Datenformatierung und -transformation:** In einigen Fällen müssen Daten in ein anderes Format oder eine andere Struktur umgewandelt werden. Dies kann bedeuten, dass Daten von einem breiten Format, in dem Merkmale in separaten Spalten vorliegen, in ein langes Format umgewandelt werden, oder dass kategorische Variablen in numerische Werte umgewandelt werden müssen. Die Datenformatierung und -transformation zielt darauf ab, die Daten in eine geeignetere Form zu bringen, um eine bessere Leistung der Modelle zu ermöglichen. (Siehe Codebeispiel)

[pandas/wide\_to\_long](https://pandas.pydata.org/docs/reference/api/pandas.wide\_to\_long.html)

Die Datenbereinigung ist ein iterativer Prozess, der oft mehrere Durchläufe erfordert, um sicherzustellen, dass die Daten in einem akzeptablen Zustand sind. Ein sorgfältig durchgeführter Datenbereinigungsprozess trägt erheblich zur Verbesserung der Qualität von Machine Learning-Modellen bei und trägt dazu bei, genaue Vorhersagen zu treffen. Es ist ein unverzichtbarer Schritt in der Datenanalyse und Modellentwicklung.

Hier ist ein Codebeispiel zur Bereinigung von Daten im Iris-Datensatz:

```python
import pandas as pd

# Laden des Iris-Datensatzes
from sklearn.datasets import load_iris
iris = load_iris()
X, y = iris.data, iris.target

# Auffüllen von fehlenden Werten (falls vorhanden)
df = df.fillna(df.mean())

# Erstellen eines Pandas DataFrame
df = pd.DataFrame(data=X, columns=iris.feature_names)

# Entfernen von Duplikaten
df = df.drop_duplicates()

# Identifizieren und Behandeln von Ausreißern
# Hier verwenden wir die Z-Score-Methode, um Ausreißer zu identifizieren und zu entfernen.
from scipy import stats
z_scores = stats.zscore(df)
abs_z_scores = np.abs(z_scores)
filtered_entries = (abs_z_scores < 3).all(axis=1)
df = df[filtered_entries]

# Konsistenz sicherstellen
# Hier standardisieren wir die Spaltennamen, um sicherzustellen, dass sie konsistent sind.
df.columns = df.columns.str.lower()

# Datenformatierung und -transformation
# Angenommen, der Iris-Datensatz hat ein breites Format, bei dem jede Blumenart in separaten Spalten für Merkmale wie Sepal Length, Sepal Width, etc. vorliegt
# Wir möchten den Datensatz in ein langes Format umwandeln, wobei jede Merkmalsvariable in einer einzigen Spalte gespeichert wird

# Zuerst konvertieren wir den Iris-Datensatz in einen Pandas DataFrame
df = pd.DataFrame(data=X, columns=iris.feature_names)
df['species'] = iris.target  # Fügen wir eine Spalte für die Art der Blume hinzu

# Dann verwenden wir die Funktion 'melt', um das breite Format in ein langes Format umzuwandeln
df_long = pd.melt(df, id_vars=['species'], var_name='feature', value_name='value')

# Auffüllen von fehlenden Werten erneut (falls nach dem Entfernen von Ausreißern noch vorhanden)
# Nachdem Ausreißer entfernt wurden, können erneut fehlende Werte auftreten, daher füllen wir diese erneut mit dem Durchschnittswert auf.
df = df.fillna(df.mean())
```

#### 1.2.3 Datenaufteilung (**Datasplitting)**

Die Aufteilung der Daten in Trainings-, Validierungs- und Testdaten ist entscheidend, um sicherzustellen, dass das Modell nicht nur die Trainingsdaten lernt und auf diese Überangepasst wird (Overfitting), sondern auch gut auf neuen, unbekannten Daten generalisiert (Generalisierungsfähigkeit). Diese Praxis trägt dazu bei, realistische Bewertungen der Modellleistung in verschiedenen Situationen zu erhalten.

1. **Trainingsdatensatz:**
   * Der Trainingsdatensatz wird verwendet, um das Modell zu trainieren. Das bedeutet, dass die Modellparameter basierend auf diesen Daten angepasst werden, um Muster und Beziehungen in den Trainingsdaten zu lernen.
   * Der Großteil der verfügbaren Daten wird oft dem Trainingsdatensatz zugewiesen, typischerweise etwa 70-80% der Gesamtdaten.
2. **Validierungsdatensatz:**
   * Der Validierungsdatensatz dient zur Feinabstimmung von Hyperparametern und zur Vermeidung von Überanpassung (Overfitting). Während des Trainingsprozesses wird das Modell auf den Trainingsdaten trainiert und auf dem Validierungsdatensatz überprüft. Die Leistung auf dem Validierungsdatensatz beeinflusst die Anpassung des Modells, um eine optimale Generalisierung zu gewährleisten.
   * Normalerweise wird ein kleiner Teil des Trainingsdatensatzes, etwa 10-20%, für das Validierungsset verwendet.
3. **Testdatensatz:**
   * Der Testdatensatz wird verwendet, um die Leistung des Modells zu evaluieren, nachdem es auf dem Trainingsdatensatz trainiert wurde. Das Testset enthält Daten, die das Modell während des Trainingsprozesses nicht gesehen hat, und ermöglicht eine objektive Bewertung der Modellfähigkeiten auf unbekannten Daten.
   * Üblicherweise wird etwa 10-20% der Daten für das Testset reserviert.

```python
from sklearn.model_selection import train_test_split

# Aufteilung des Datensatzes in Trainings- und Testdaten
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Aufteilung des Trainingsdatensatzes in Trainings- und Validierungsdaten
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# - X_train und y_train für das Training des Modells
# - X_val und y_val für die Validierung des Modells während des Trainings
# - X_test und y_test für die finale Evaluation des trainierten Modells
```

Nach diesen Datenvorverarbeitungsschritten sind die Daten bereit für das Training eines Machine-Learning-Modells.

[Quelle/weitere Infos](https://medium.com/@datasciencewizards/a-guide-to-data-splitting-in-machine-learning-49a959c95fa1), [Quelle/weitere Infos 2](https://medium.com/@tubelwj/five-methods-for-data-splitting-in-machine-learning-27baa50908ed)

#### 2.2.4 **Datenexploration**

Die Datenexploration ist ein entscheidender Schritt im Datenanalyseprozess, der die Grundlage für fundierte Entscheidungen und die Entwicklung erfolgreicher Modelle bildet. Die Hauptkomponenten dieses beispielhaften Prozesses sind mit Codebeispielen im Folgenden zu finden.

*   **Statistische Kennzahlen:** Statistische Kennzahlen wie der Durchschnitt, Median und die Standardabweichung bieten eine grundlegende Zusammenfassung der Datenverteilung. Sie ermöglichen einen ersten Überblick über die zentrale Tendenz und die Streuung der Daten.

    ```python
    import numpy as np

    data = [10, 20, 30, 40, 50]
    mean = np.mean(data)  # Durchschnitt berechnen
    median = np.median(data)  # Median berechnen
    std_dev = np.std(data)  # Standardabweichung berechnen

    print("Durchschnitt:", mean)
    print("Median:", median)
    print("Standardabweichung:", std_dev)
    ```
*   **Datenvisualisierung:** Die Verwendung von visuellen Darstellungen wie Histogrammen, Boxplots und Scatterplots hilft bei der Identifizierung von Mustern, Ausreißern und Verteilungen. Diese visuellen Elemente erleichtern das Verständnis der Datenstruktur und ermöglichen die Erkennung potenzieller Zusammenhänge.

    ```python
    import matplotlib.pyplot as plt

    data = [10, 20, 30, 40, 50]
    plt.hist(data, bins=5)
    plt.xlabel('Werte')
    plt.ylabel('Häufigkeit')
    plt.title('Histogramm der Daten')
    plt.show()
    ```
*   **Korrelationsanalyse:** Die Untersuchung von Korrelationen zwischen verschiedenen Merkmalen offenbart potenzielle Abhängigkeiten und Beziehungen. Dies ist entscheidend, um zu verstehen, wie sich verschiedene Variablen in den Daten verhalten.

    ```python
    import pandas as pd

    data = pd.DataFrame({'A': [1, 2, 3, 4, 5],
                         'B': [5, 4, 3, 2, 1],
                         'C': [1, 3, 5, 7, 9]})
    correlation_matrix = data.corr()
    print(correlation_matrix)
    ```
*   **Mustererkennung:** Die Suche nach Mustern, Saisonalitäten, Clustern oder Anomalien trägt zur Erkenntnisgewinnung bei. Dieser Schritt ermöglicht die Identifizierung von wiederkehrenden Trends oder ungewöhnlichen Ereignissen in den Daten.

    ```python
    from sklearn.cluster import KMeans

    # Annahme: data ist ein DataFrame mit den Daten
    kmeans = KMeans(n_clusters=3)
    kmeans.fit(data)
    clusters = kmeans.predict(data)
    print(clusters)
    ```
*   **Hypothesenbildung:** Die Formulierung von Hypothesen über die Daten legt den Grundstein für spätere Analysen und Experimente. Dieser explorative Ansatz fördert ein tiefes Verständnis der zugrunde liegenden Strukturen und Zusammenhänge.

    Beispielhaft für eine Formulierung einer Hypothese über den Zusammenhang zwischen Luftqualitätsindikatoren und Asthmafällen ist: "An Tagen mit hoher Luftverschmutzung steigt die Anzahl der Asthmafälle.”
*   **Fehler- und Inkonsistenzprüfung:** Die systematische Überprüfung auf Fehler, fehlende Werte oder Inkonsistenzen gewährleistet die Qualität der Daten. Eine sorgfältige Bereinigung von Datenfehlern ist entscheidend, um zuverlässige Analysen durchzuführen.

    ```python
    import pandas as pd

    # Annahme: data ist ein DataFrame
    missing_values = data.isnull().sum()
    print(missing_values)
    ```
* **Fachwissen einbeziehen:** Die Zusammenarbeit mit Domänenexperten ist vorteilhaft, um die Daten im richtigen Kontext zu interpretieren. Das Fachwissen hilft dabei, potenzielle Einflussfaktoren zu identifizieren und die Ergebnisse besser zu verstehen. Z. B. ist die Zusammenarbeit mit einem Umweltwissenschaftler, um Sensordaten zur Luftqualität zu interpretieren und den potenziellen Einfluss auf Asthmafälle zu verstehen vorteilhaft.

Die Datenexploration ist ein iterativer und dynamischer Prozess, der die Grundlage für die Auswahl geeigneter Merkmale, die Modellentwicklung und die erfolgreiche Anwendung von Machine Learning-Algorithmen bildet. Ein gründliches Datenverständnis ist der Schlüssel zu effektiven Entscheidungen und präzisen Vorhersagen.

[IBM/What is EDA?](https://www.ibm.com/topics/exploratory-data-analysis) [Kaggle/Comprehensive data exploration with Python](https://www.kaggle.com/code/pmarcelino/comprehensive-data-exploration-with-python)

#### 2.2.5 **Feature Engineering**

Feature Engineering ist der Prozess, bei dem neue Merkmale aus den vorhandenen Daten erstellt werden, um die Modellleistung zu verbessern. Im Fall des Iris-Datensatzes, der bereits gut strukturierte Merkmale enthält, ist Feature Engineering nicht notwendig. Um Feature Engineering in Python mit einem anderen Open-Source-Datensatz durchzuführen, bietet sich der "Breast Cancer Wisconsin (Diagnostic)"-Datensatz als eine Option an.

[Quelle/weitere Infos](https://www.heavy.ai/technical-glossary/feature-engineering), [Quelle/weitere Infos 2](https://www.projectpro.io/article/8-feature-engineering-techniques-for-machine-learning/423)

```python
import pandas as pd
from sklearn.datasets import load_breast_cancer

# Laden des Breast Cancer-Datensatzes
data = load_breast_cancer()

# Erstellen eines Pandas DataFrame
df = pd.DataFrame(data.data, columns=data.feature_names)

# Hinzufügen der Zielvariable 'diagnosis'
df['diagnosis'] = data.target

# Feature Engineering: Beispiel für Berechnung eines neuen Merkmals
# Hier berechnen wir den Durchschnitt der 'smoothness_mean' für alle Proben
df['smoothness_mean_avg'] = df.groupby('diagnosis')['smoothness_mean'].transform('mean')

# Aufteilung des Datensatzes in Trainings- und Testdaten
from sklearn.model_selection import train_test_split

X = df.drop('diagnosis', axis=1)
y = df['diagnosis']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

