# 1.4 Semi-Überwachtes Lernen (Supervised Learning)

Semi-Überwachtes Lernen ist ein Bereich des maschinellen Lernens, der Merkmale sowohl aus überwachten als auch unüberwachten Lernansätzen vereint. In diesem Ansatz werden Trainingsdaten verwendet, die teilweise gelabelt und teilweise ungelabelt sind. Im Gegensatz zum vollständig überwachten Lernen, bei dem alle Trainingsdaten gelabelt sind, erlaubt Semi-Supervised Learning Modelle, von der zusätzlichen Information aus den ungelabelten Daten zu profitieren. Darüber hinaus werden fortgeschrittenere Techniken wie Self-Training und Co-Training vorgestellt, die es Semi-Supervised-Learning-Modellen ermöglichen, die Qualität und Menge der gelabelten Daten schrittweise zu verbessern, indem sie Informationen aus ungelabelten Daten nutzen oder Modelle gemeinsam trainieren.

Der Prozess kann in mehreren Schritten erfolgen:

1. **Üblicher überwachter Lernprozess:** Beginnend mit den gelabelten Daten wird das Modell initial trainiert, indem es die vorhandenen Labels nutzt, um Muster und Zusammenhänge zu lernen.
2. **Erweiterung durch ungelabelte Daten:** Nach dem überwachten Training wird das Modell mit den ungelabelten Daten weitertrainiert. Hierbei versucht es, die zugrunde liegenden Strukturen der Daten zu erfassen und seine Vorhersagefähigkeiten zu verbessern.
3. **Verbesserung durch iteratives Training:** Der Prozess der Nutzung gelabelter und ungelabelter Daten kann iterativ wiederholt werden. Das Modell wird weiter verbessert, indem es zwischen gelabelten und ungelabelten Daten wechselt.

**Vorteile:**

* Effiziente Nutzung von gelabelten und unlabeled Daten: Durch die Kombination von gelabelten und unlabeled Daten können Semi-Supervised-Learning-Modelle wertvolle Informationen aus einer umfangreichen Menge von Daten gewinnen, was zu einer effizienteren Nutzung der verfügbaren Ressourcen führt
* Verbesserte Modellleistung: Semi-Supervised-Learning-Modelle können oft bessere Vorhersagegenauigkeit erzielen als reine Supervised- oder Unsupervised-Learning-Modelle, da sie von einer breiteren Palette von Daten profitieren können
* Reduzierte Anfälligkeit für Überanpassung: Durch die Integration von Informationen aus unlabeled Daten können Semi-Supervised-Learning-Modelle robuster gegenüber Overfitting sein, da sie nicht ausschließlich auf gelabelte Daten angewiesen sind

**Nachteile:**

* Die Qualität der Informationen aus unlabeled Daten kann erheblich variieren und ist stark von der Repräsentativität der Daten abhängig
* Semi-Supervised Learning-Modelle müssen robust gegenüber Fehlklassifikationen auf unlabeled Daten sein, um sicherzustellen, dass die Qualität der erweiterten gelabelten Daten nicht beeinträchtigt wird

Insgesamt bietet Semi-Supervised Learning eine leistungsstarke Methode, um das Beste aus beiden Welten, gelabelte und unlabeled Daten, zu kombinieren und Modelle mit verbesserten Leistungen zu erstellen. Semi-Supervised Learning findet in verschiedenen Anwendungsbereichen Anwendung, insbesondere wenn das Sammeln von gelabelten Daten teuer oder zeitaufwändig ist.

[Quelle/weitere Infos](https://www.ibm.com/topics/semi-supervised-learning), [Quelle/weitere Infos 2](https://www.altexsoft.com/blog/semi-supervised-learning/), [Quelle/weitere Infos 3](https://www.v7labs.com/blog/semi-supervised-learning-guide)

#### 1.4.2 Self-Training

Eine häufig verwendete Technik im Semi-Supervised Learning ist das sogenannte Self-Training. Hierbei wird das Modell mit den gelabelten Daten trainiert und dann auf den ungelabelten Daten Vorhersagen getroffen. Diese Vorhersagen werden als zusätzliche “Pseudo gelabelte Daten” betrachtet, und der Trainingsprozess wird iterativ wiederholt. Hier ist der Trainingsprozess nochmals schrittweise dargestellt:

1. **Initialisierung:** Trainiere ein Basismodell auf einem kleinen, gelabelten, Datensatz.
2. **Vorhersage:** Verwende das Basismodell, um Vorhersagen für ungelabelte Daten zu machen.
3. **Auswahl:** Wähle die ungelabelten Datenpunkte mit der höchsten Vorhersagekonfidenz aus.
4. **Labelzuweisung:** Weise diesen ausgewählten Datenpunkten basierend auf den Vorhersagen des Modells das Label zu (Pseudolabel).
5. **Aktualisierung:** Erweitere den ursprünglichen Trainingsdatensatz um die neu gelabelten Daten.
6. **Wiederholung:** Trainiere das Modell erneut mit dem erweiterten Datensatz.
7. **Iteration:** Wiederhole die Schritte 2 bis 6 für eine festgelegte Anzahl von Iterationen oder bis keine signifikante Verbesserung der Modellleistung mehr erreicht wird.

Semi-Supervised Learning ermöglicht es, die Vorteile sowohl von gelabelten als auch von ungelabelten Daten zu nutzen, was insbesondere in Szenarien mit begrenzten gelabelten Daten von großem Nutzen ist.

**Vorteile:**

* Effiziente Nutzung von ungelabelten Daten: Self-Training ermöglicht es, Informationen aus ungelabelten Daten zu nutzen, um die Leistung des Modells zu verbessern, ohne dass zusätzliche manuelle Annotationen erforderlich sind.
* Erweiterung des Trainingsdatensatzes: Durch die Verwendung von Pseudolabels werden die Trainingsdaten sukzessive erweitert, was zu einer besseren Generalisierungsfähigkeit des Modells führen kann.
* Anpassung an sich ändernde Datenverteilungen: Self-Training kann dazu beitragen, dass das Modell sich an sich ändernde Datenverteilungen anpasst, da es kontinuierlich mit neuen Daten trainiert wird.

**Nachteile:**

* Risiko von Fehlerakkumulation: Da die Pseudolabels auf den Vorhersagen des Modells basieren, besteht das Risiko, dass Fehler in den Vorhersagen des Modells akkumulieren und die Qualität der erweiterten Trainingsdaten beeinträchtigen.
* Sensitivität gegenüber Unsicherheit der Vorhersagen: Self-Training ist empfindlich gegenüber der Unsicherheit der Vorhersagen des Modells. Bei ungenauen Vorhersagen kann die Qualität der Pseudolabels abnehmen, was sich negativ auf die Leistung des Modells auswirken kann.

```python
import tensorflow as tf
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np

# Laden des Digit-Datensatzes
digits = load_digits()
X = digits.data
y = digits.target

# Aufteilen der Daten in gelabelte und unlabeled Teile
X_labeled, X_unlabeled, y_labeled, _ = train_test_split(X, y, test_size=0.9, random_state=42)

# Definition eines einfachen Klassifikationsmodells
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Kompilieren des Modells
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Training mit gelabelten Daten
model.fit(X_labeled, y_labeled, epochs=10, batch_size=32, validation_split=0.2)

# Selbst-Training mit unlabeled Daten
for _ in range(5):
    # Vorhersagen auf unlabeled Daten
    pseudo_labels = model.predict(X_unlabeled)
    # Auswahl der zuverlässigsten Vorhersagen
    confident_predictions = np.max(pseudo_labels, axis=1) > 0.8
    # Hinzufügen der zuverlässigen Vorhersagen zu den gelabelten Daten
    X_labeled = np.concatenate([X_labeled, X_unlabeled[confident_predictions]])
    y_labeled = np.concatenate([y_labeled, np.argmax(pseudo_labels[confident_predictions], axis=1)])
    # Training mit den erweiterten gelabelten Daten
    model.fit(X_labeled, y_labeled, epochs=5, batch_size=32, validation_split=0.2)

# Evaluierung auf Testdaten
test_accuracy = accuracy_score(y_test, np.argmax(model.predict(X_test), axis=1))
print(f'Testgenauigkeit nach Selbsttraining: {test_accuracy}')
```

Das obige Beispiel illustriert den Self-Training-Ansatz im Semi-Supervised Learning mit einem einfachen Klassifikationsmodell und dem MNIST-Datensatz.

#### **1.4.3 Co-Training**

Eine weitere Strategie im Bereich des Semi-Supervised Learning ist das Co-Training. Hierbei werden mehrere Modelle unabhängig voneinander trainiert, jedoch auf verschiedenen Teilmengen der Daten, einschließlich der gelabelten und unlabeled Daten. Diese Modelle tauschen dann Informationen aus und lernen voneinander. Diese Methode ist besonders effektiv, wenn verschiedene Modelle unterschiedliche Aspekte der Daten erfassen. Damit ist gemeint, dass jedes Modell spezifische Merkmale oder Informationen der Daten fokussiert, sodass durch den Informationsaustausch zwischen den Modellen ein umfassenderes Verständnis und bessere Vorhersagen erzielt werden können.

* **Unabhängiges Training auf verschiedenen Teilmengen:** Mehrere Modelle werden auf separaten Teilmengen der Daten trainiert, wobei jedes Modell nur einen Teil der verfügbaren Informationen nutzt.
* **Austausch von Informationen zwischen den Modellen:** Nach dem unabhängigen Training tauschen die Modelle Informationen aus, indem sie Vorhersagen oder Wahrscheinlichkeiten für die nicht gelabelten Daten des anderen Modells verwenden.
* **Konsensbildung oder Gewichtsübertragung:** Die ausgetauschten Informationen werden verwendet, um konsistente Vorhersagen zu generieren oder Gewichte zu aktualisieren, um die Modelle zu verbessern.
* **Iterativer Prozess:** Der Informationsaustausch und das Training werden iterativ durchgeführt, um die Leistung der Modelle schrittweise zu verbessern.
* **Validierung und Evaluierung:** Die kombinierten Modelle werden auf ihre Leistungsfähigkeit anhand eines separaten Validierungsdatensatzes oder durch Cross-Validation evaluiert, um sicherzustellen, dass sie effektiv auf neuen, ungelabelten Daten generalisieren können.

Im Co-Training-Prozess werden Modelle auf verschiedenen Teilmengen trainiert und tauschen anschließend Informationen aus, indem sie Vorhersagen auf den Daten des jeweils anderen Modells nutzen.

**Vorteile:**

* **Effektive Nutzung von unterschiedlichen Perspektiven:** Durch das Training mehrerer Modelle auf verschiedenen Teilmengen der Daten und den anschließenden Austausch von Informationen können verschiedene Aspekte der Daten erfasst und genutzt werden, was zu einem umfassenderen Verständnis führt.
* **Verbesserte Generalisierungsfähigkeit:** Der Informationsaustausch zwischen den Modellen ermöglicht es, konsistente Vorhersagen zu generieren und die Modelle zu verbessern, was letztendlich zu einer besseren Generalisierungsfähigkeit auf neuen, ungelabelten Daten führt.
* **Robustheit gegenüber Rauschen und Fehlern:** Durch die Konsensbildung oder den Informationsaustausch können Fehler und Rauschen in den Vorhersagen reduziert werden, was zu robusteren Modellen führt.
* **Flexibilität und Anpassungsfähigkeit:** Co-Training ist flexibel und kann auf verschiedene Problemstellungen und Datensätze angewendet werden, da es sich an die spezifischen Merkmale der Daten anpassen kann.

**Nachteile:**

* **Notwendigkeit mehrerer Modelle:** Co-Training erfordert das Training und die Wartung mehrerer Modelle, was zu einem höheren Rechenaufwand führen kann, insbesondere bei großen Datensätzen.
* **Komplexität des Informationsaustauschs:** Der Informationsaustausch zwischen den Modellen kann komplex sein und erfordert möglicherweise zusätzliche Mechanismen zur Konsensbildung oder Gewichtsübertragung, was die Implementierung und das Verständnis erschweren kann.
* **Abhängigkeit von der Diversität der Modelle:** Die Effektivität von Co-Training hängt davon ab, dass die trainierten Modelle unterschiedliche Aspekte der Daten erfassen. Wenn die Modelle zu ähnlich sind, kann der Informationsaustausch weniger effektiv sein.code

```python
import tensorflow as tf
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np

# Laden des Digit-Datensatzes
digits = load_digits()
X = digits.data
y = digits.target

# Aufteilen der Daten in zwei Teilmengen
X1, X2, y1, y2 = train_test_split(X, y, test_size=0.5, random_state=42)

# Definition von zwei unterschiedlichen Modellen
model1 = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

model2 = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Kompilieren der Modelle
model1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Training der Modelle auf verschiedenen Teilmengen
model1.fit(X1, y1, epochs=10, batch_size=32, validation_split=0.2)
model2.fit(X2, y2, epochs=10, batch_size=32, validation_split=0.2)

# Austausch von Vorhersagen und erneutes Training
for _ in range(5):
    # Vorhersagen von Modell 1 auf Datensatz von Modell 2
    pseudo_labels_2 = model1.predict(X2)
    # Auswahl der zuverlässigsten Vorhersagen
    confident_predictions_2 = np.max(pseudo_labels_2, axis=1) > 0.8
    # Hinzufügen der zuverlässigen Vorhersagen zu den gelabelten Daten von Modell 2
    X2_labeled = np.concatenate([X2, X2[confident_predictions_2]])
    y2_labeled = np.concatenate([y2, np.argmax(pseudo_labels_2[confident_predictions_2], axis=1)])
    # Training von Modell 2 mit den erweiterten gelabelten Daten
    model2.fit(X2_labeled, y2_labeled, epochs=5, batch_size=32, validation_split=0.2)

    # Analoger Prozess für Modell 1
    pseudo_labels_1 = model2.predict(X1)
    confident_predictions_1 = np.max(pseudo_labels_1, axis=1) > 0.8
    X1_labeled = np.concatenate([X1, X1[confident_predictions_1]])
    y1_labeled = np.concatenate([y1, np.argmax(pseudo_labels_1[confident_predictions_1], axis=1)])
    model1.fit(X1_labeled, y1_labeled, epochs=5, batch_size=32, validation_split=0.2)

# Evaluierung auf Testdaten
test_accuracy_1 = accuracy_score(y_test, np.argmax(model1.predict(X_test), axis=1))
test_accuracy_2 = accuracy_score(y_test, np.argmax(model2.predict(X_test), axis=1))

print(f'Testgenauigkeit von Modell 1: {test_accuracy_1}')
print(f'Testgenauigkeit von Modell 2: {test_accuracy_2}')
```

