# 1.6 Bestärkendes Lernen (Reinforcement Learning)

### 1.6 Bestärkendes **Lernen (Reinforcement Learning)**

Reinforcement Learning (RL) ist ein mächtiger Zweig des maschinellen Lernens, der sich auf die Entwicklung von autonomen Agenten konzentriert, die lernen, durch Interaktion mit ihrer Umgebung die besten Entscheidungen zu treffen. Im Wesentlichen handelt es sich um das Lernen durch Versuch und Irrtum. Der Agent interagiert mit seiner Umgebung, wählt Aktionen aus und erhält Belohnungen als Rückmeldung. Das Ziel ist, eine Policy (Richtlinie) zu entwickeln, die die besten Aktionen für jeden Zustand vorgibt.

<figure><img src="../.gitbook/assets/Reinforcement_Learning.svg" alt=""><figcaption><p>Abbildung 18: Grundprinzip des Reinforcement Learnings</p></figcaption></figure>

**Exploration vs. Exploitation**

In RL steht der Agent oft vor dem Dilemma zwischen Exploration und Exploitation. Das bedeutet, der Agent muss entscheiden, ob er bereits gelernte Aktionen auswählen sollte, die ihm wahrscheinlich Belohnungen einbringen (Exploitation), oder ob er neue, unbekannte Aktionen ausprobieren sollte, um mehr über die Umgebung zu erfahren (Exploration). Die richtige Balance zwischen diesen beiden Aspekten ist entscheidend, um eine effektive Richtlinie zu entwickeln. Dabei kann die Umgebung oder das Label dynamisch erzeugt werden, indem die Belohnungen als "gut" oder "schlecht" bewertet werden.

**Policy- und Value-Funktionen**

* **Policy-Funktion:** Eine Richtlinie beschreibt die Wahrscheinlichkeit, mit der der Agent in verschiedenen Zuständen verschiedene Aktionen auswählt. Sie ist der Schlüssel zur Steuerung des Agenten und bestimmt sein Verhalten in der Umgebung.
* **Value-Funktion:** Wertfunktionen bewerten, wie "gut" es ist, sich in einem bestimmten Zustand zu befinden oder eine bestimmte Aktion in einem Zustand auszuführen. Sie helfen dem Agenten, die besten Entscheidungen zu treffen, indem sie die erwartete Belohnung für verschiedene Aktionen quantifizieren.

Reinforcement Learning ist ein faszinierendes Feld mit Anwendungen in Bereichen wie Spielentwicklung, Robotik und autonomen Systemen.

Einige der bekanntesten Algorithmen sind:

1. **Q-Learning:** Ein grundlegender und weit verbreiteter Algorithmus im Reinforcement Learning, der auf der Schätzung von Q-Werten basiert. Q-Learning ist besonders nützlich für Probleme, bei denen die Umgebung nicht bekannt ist.
2. **Deep Q-Networks (DQN):** Eine Erweiterung des Q-Learning-Algorithmus, der künstliche neuronale Netze einsetzt, um komplexe und hochdimensionale Eingabedaten zu verarbeiten. DQN hat sich insbesondere in der Anwendung auf Videospiele und komplexe Umgebungen bewährt.
3. **Policy Gradient Algorithms:** Diese Algorithmen konzentrieren sich auf die direkte Optimierung der Richtlinienfunktion, um die Leistung zu verbessern. Beispiele hierfür sind der REINFORCE-Algorithmus und der Proximal Policy Optimization (PPO)-Algorithmus.
4. **Actor-Critic Algorithms:** Diese Algorithmen kombinieren Elemente der Richtlinienoptimierung und der Wertoptimierung, indem sie sowohl einen Aktor als auch einen Kritiker im Lernalgorithmus verwenden. Beispiele sind der A3C (Asynchronous Advantage Actor-Critic) Algorithmus und der A2C (Advantage Actor-Critic) Algorithmus.
5. **Deep Deterministic Policy Gradient (DDPG):** Dies ist ein Algorithmus, der speziell für kontinuierliche Aktionsräume entwickelt wurde und auf der Kombination von deterministischen Richtlinien und dem Konzept der Wertfunktionen basiert.
6. **SARSA (State-Action-Reward-State-Action):** Ein weiterer wichtiger Algorithmus, der ähnlich wie Q-Learning ist, jedoch den nächsten Schritt des Agenten basierend auf einer bestimmten Richtlinie berücksichtigt.

[Quelle/weitere Infos](https://aws.amazon.com/de/what-is/reinforcement-learning/), [Quelle/weitere Infos 2](https://www.geeksforgeeks.org/what-is-reinforcement-learning/), [Quelle/weitere Infos 3](https://towardsdatascience.com/reinforcement-learning-101-e24b50e1d292)
