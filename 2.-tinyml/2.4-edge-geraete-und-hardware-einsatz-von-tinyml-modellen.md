# 2.4 Edge-Geräte und Hardware-Einsatz von TinyML-Modellen

Bevor ein TinyML-Modell auf Edge-Geräten bereitgestellt werden kann, muss sichergestellt werden, dass es für die Verwendung in einer ressourcenbeschränkten Umgebung optimiert ist. Hier sind einige wichtige Schritte zur Vorbereitung eines Modells für die Ausführung auf einem Edge-Gerät:

#### 2.4.1 Modellkomprimierung und -optimierung

Bevor ein neuronales Netz für die Ausführung auf einem Edge-Gerät konvertiert wird, sollte das Modell quantisiert und optimiert werden (Siehe auch Kapitel 3.3). Dies umfasst Techniken wie bspw. Pruning oder Knowledge Distillation, die dazu beitragen, die Modellgröße zu reduzieren und die Effizienz auf Edge-Geräten zu steigern. Diese Optimierungsschritte können sehr entscheidend sein, um sicherzustellen, dass das Modell auf ressourcenbeschränkten Geräten die gewünschten Inferenzzeiten erreicht.

#### 2.4.2 **Modelkonvertierung zu TensorFlow Lite**

TensorFlow Lite (TFLite) wurde entwickelt, um Machine-Learning-Modelle auf Edge-Geräten auszuführen, die normalerweise über begrenzte Rechenleistung und Speicher verfügen. Hier sind einige Gründe, warum die Verwendung von TFLite sinnvoll ist:

* **Effizienz:** TFLite ist auf Effizienz ausgelegt und bietet optimierte Berechnungen, um die Ausführung auf ressourcenbeschränkten Geräten zu beschleunigen.
* **Plattformunabhängigkeit:** TFLite unterstützt verschiedene Hardwareplattformen und Betriebssysteme, was die Bereitstellung auf verschiedenen Edge-Geräten erleichtert.
* **Unterstützung für spezielle Hardware:** TFLite ermöglicht die Nutzung spezialisierter Hardwarebeschleuniger, um die Inferenz noch schneller zu machen.

**Schritte zur Modelkonvertierung**

Die Konvertierung eines TensorFlow Modells in das TFLite-Format umfasst die folgenden Schritte:

1. **Modellvorbereitung:** Laden des Modells und die Konfiguration von Eingabe- und Ausgabe-Tensoren.
2.  **Modellkonvertierung und Quantisierung:** Verwenden der TensorFlow-Bibliothek, um das Modell in das TFLite-Format zu konvertieren. [Hier ist ein Beispiel](https://github.com/Hahn-Schickard/AutoFlow/blob/main/src/converter/convert\_keras\_to\_cc.py):&#x20;

    ```python
    from tensorflow import lite
    import tensorflow as tf
    import sys

    def convert_model_to_tflite(keras_model_dir, project_dir, model_name,
                                optimization, data_loader_path, quant_dtype,
                                separator, decimal, csv_target_label):
        """
        A keras model get's converter into a TensorFlow lite model.

        Args:
            keras_model_dir:     Path of the keras model
            project_dir:         Path of the project
            model_name:          Name of converted .tflite file
            optimization:        Selected optimization algorithms
            data_loader_path:    Path of the folder or file with the training data
            quant_dtype:         Data type to quantize to
            separator:           Separator for reading a CSV file
            decimal:             Decimal for reading a CSV file
            csv_target_label:    Target label from the CSV file

        Return:
            model_input_shape:    Shape of the inputdata of the model
            model_output_neurons: Number of neurons in the output layer
        """
        print("convert_model_to_tflite function called")
        keras_model = keras_model_dir
        keras_model = tf.keras.models.load_model(keras_model)
        model_input_shape = keras_model.input.shape
        model_output_neurons = keras_model.layers[-1].output_shape[1]
        converter = lite.TFLiteConverter.from_keras_model(keras_model)

        if "Quantization" in optimization:

            converter.optimizations = [tf.lite.Optimize.DEFAULT]
            
            print("Quantization type:", quant_dtype)
            if "int8 only" in quant_dtype:
                global x_train
                x_train = dataloader_quantization(
                    data_loader_path, keras_model.input.shape[1],
                    keras_model.input.shape[2], separator, decimal,
                    csv_target_label)
                x_train = tf.cast(x_train, tf.float32)
                x_train = tf.data.Dataset.from_tensor_slices(x_train).batch(1)

                converter.representative_dataset = representative_dataset

                supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
                converter.target_spec.supported_ops = supported_ops
                converter.inference_input_type = tf.int8
                converter.inference_output_type = tf.int8

        tflite_model = converter.convert()
        open(project_dir + "/" + model_name + ".tflite", "wb").write(tflite_model)
        return model_input_shape, model_output_neurons
    ```
3. **TFLite-Modell speichern:** Das konvertierte TFLite-Modell wird in einer Datei gespeichert, die auf Edge-Geräten geladen und ausgeführt werden kann.

**Modelkonvertierung zu C++-Modell**

In einigen Fällen kann es erforderlich sein, das Modell in eine C++-Darstellung zu konvertieren, um es auf bestimmten Hardwareplattformen wie bspw. Mikrocontrollern auszuführen. Wie das Modell in ein C++-Modell konvertiert werden kann, wird im folgenden mit einem [Beispiel](https://github.com/Hahn-Schickard/AutoFlow/blob/main/src/converter/convert\_keras\_to\_cc.py) in Python gezeigt:

```python
from tensorflow import lite
import tensorflow as tf
import sys

def convert_model_to_cpp(model_name, project_dir):
    """
    A TensorFlow lite model get's converter into a C array model.

    Args:
        model_name:     Name of the model
        project_dir:    Directory of the project where the C array model
                        should be stored
    """
    print("convert_model_to_cpp function called")
    with open(project_dir + "/" + model_name + '.tflite', 'rb') as f:
        content = f.read().hex()
        result = bytearray.fromhex(content)
        with open(project_dir + "/src/" + model_name + "_data.cpp", "wb") as w:
            values_in_row = 0
            num_values = 0

            w.write(bytearray('#include "' + model_name + '_data.h"\\n'
                              "\\n"
                              "// We need to keep the data array aligned on"
                              "some architectures.\\n"
                              "#ifdef __has_attribute\\n"
                              "#define HAVE_ATTRIBUTE(x) __has_attribute(x)\\n"
                              "#else\\n"
                              "#define HAVE_ATTRIBUTE(x) 0\\n"
                              "#endif\\n"
                              "#if HAVE_ATTRIBUTE(aligned) || (defined("
                              "__GNUC__) && !defined(__clang__))\\n"
                              "#define DATA_ALIGN_ATTRIBUTE __attribute__("
                              "(aligned(4)))\\n"
                              "#define DATA_ALIGN_ATTRIBUTE __attribute__("
                              "(aligned(4)))\\n"
                              "#else\\n"
                              "#define DATA_ALIGN_ATTRIBUTE\\n"
                              "#endif\\n"
                              "\\n"

                              "const unsigned char " + model_name + "_tflite[]"
                              " DATA_ALIGN_ATTRIBUTE = {\\n    ", 'utf-8'))

            for value in result:
                num_values += 1
                values_in_row += 1
                value = "0x{:02x}".format(value)

                if values_in_row == 1:
                    w.write(bytearray(value, 'utf-8'))
                elif values_in_row == 12:
                    w.write(bytearray(", " + str(value) + ",\\n    ", 'utf-8'))
                    values_in_row = 0
                else:
                    w.write(bytearray(', ' + str(value), 'utf-8'))

            w.write(bytearray("};\\nconst int " + model_name + "_tflite_len = "
                              + str(num_values) + ";", 'utf-8'))

    with open(project_dir + "/inc/" + model_name + "_data.h", "w") as f:
        f.write('// This is a standard TensorFlow Lite model file '
                'that has been converted into a\\n'
                '// C data array, so it can be easily compiled into '
                'a binary for devices that\\n'
                "// don't have a file system.\\n"
                '\\n'
                '#ifndef TENSORFLOW_LITE_MODEL_DATA_H_\\n'
                '#define TENSORFLOW_LITE_MODEL_DATA_H_\\n'
                '\\n'
                'extern const unsigned char ' + model_name + '_tflite[];\\n'
                'extern const int ' + model_name + '_tflite_len;\\n'
                '\\n'
                '#endif')
```

#### 2.4.3 Einsatz von TensorFlow Lite Modellen auf Mikrocontrollern

Nach der Optimierung und Konvertierung des TinyML-Modells, kann es anschließend auf einem Mikrocontroller ausgeführt werden. Hier ist eine grundlegende Anleitung für das Ausführen von TensorFlow Lite-Modellen auf Mikrocontrollern:

1. **Modellvorbereitung:** Stellen Sie sicher, dass Ihr Modell geladen und für die Konvertierung bereit ist, ähnlich wie bei der TFLite-Konvertierung.
2. **TFLite-Interpreter in C++ integrieren:** Der TensorFlow Lite Interpreter wird benötigt, um das Modell in C++ auszuführen. Der Interpreter ist in der Regel in C++ geschrieben und kann in Ihr C++-Projekt integriert werden.
3. **Konfiguration von Eingabe und Ausgabe:** Die Eingabe- und Ausgabe-Tensoren müssen entsprechend des verwendeten Modells konfiguriert werden. Das hängt von den spezifischen Anforderungen des Modells ab.
4. **Inferenz ausführen:** Verwendung des TFLite-Interpreters, um die Inferenz auf den Eingabetensoren durchzuführen.
5. **Verarbeitung der Ausgabe:** Nach der Ausführung des Modells wird die Vorhersage entsprechend weiterverarbeitet und -verwendet.

Im folgenden ist ein [Codebeispiel](https://github.com/Hahn-Schickard/AutoFlow/blob/main/example/templates/arduino\_mnist/tf\_lite\_exe.cpp) aus wie dies aussehen kann:

```cpp
#include "tf_lite_exe.h"

namespace {
// Create an area of memory to use for input, output, and intermediate arrays.
constexpr int kTensorArenaSize = 100 * 1024;
uint8_t tensor_arena[kTensorArenaSize];

tflite::ErrorReporter* error_reporter = nullptr;
const tflite::Model* model = nullptr;
tflite::MicroInterpreter* interpreter = nullptr;
TfLiteTensor* input = nullptr;
TfLiteTensor* output = nullptr;

float* prediction = new float[10];
}

void setup_model() {
  static tflite::MicroErrorReporter micro_error_reporter;
  error_reporter = &micro_error_reporter;

  // Load the tflite Model
  model = tflite::GetModel(MNIST_tflite);
  if (model->version() != TFLITE_SCHEMA_VERSION) {
    error_reporter->Report(
        "Model provided is schema version %d not equal "
        "to supported version %d.",
        model->version(), TFLITE_SCHEMA_VERSION);
    return;
  }

  // This pulls in all the operation implementations we need.
  static tflite::AllOpsResolver resolver;

  // Build an interpreter to run the model with.
  static tflite::MicroInterpreter static_interpreter(
      model, resolver, tensor_arena, kTensorArenaSize, error_reporter);
  interpreter = &static_interpreter;

  // Allocate memory from the tensor_arena for the model's tensors.
  TfLiteStatus allocate_status = interpreter->AllocateTensors();
  if (allocate_status != kTfLiteOk) {
    error_reporter->Report("AllocateTensors() failed");
    return;
  }

  // Obtain pointers to the model's input and output tensors.
  input = interpreter->input(0);
  output = interpreter->output(0);

}

float* model_execute(float *input_data) {
  for (int i = 0; i < 784; ++i) {
    input->data.f[i] = *input_data;
    input_data++;
  }

  // Run inference, and report any error
  TfLiteStatus invoke_status = interpreter->Invoke();
  if (invoke_status != kTfLiteOk) {
    error_reporter->Report("Error by invoking interpreter\\n");
    return 0;
  }

  // Read the prediction from the model's output tensor
  for (int i = 0; i < 10; i++) {
    prediction[i] = output->data.f[i];
  }

  return prediction;
}
```

**Was muss beachtet werden (Libraries)**

Beachten Sie, dass Sie die erforderlichen Bibliotheken, wie **`tflite_runtime`**, auf Ihrem Mikrocontroller installieren müssen. Weitere Bibliotheken, die für Ihr Vorhaben nützlich sein könnten, sind unter [https://github.com/tensorflow/tflite-micro](https://github.com/tensorflow/tflite-micro) zu finden. Stellen Sie sicher, dass Ihr Mikrocontroller die Hardwareanforderungen des Modells erfüllt und ausreichend Speicher und Rechenleistung bietet.

#### 2.4.4 Einsatz von TensorFlow Lite Modellen auf Raspberry Pi und anderen Einplatinencomputern

Die Bereitstellung von TensorFlow Lite-Modellen auf leistungsstärkeren Einplatinencomputern wie dem Raspberry Pi erfordert einige zusätzliche Schritte:

**Codebeispiel für das Deployment**

```python
import tensorflow as tf
import numpy as np
import sys

# Path of TFLite model
tflite_model_file = '../data/mnist_model.tflite'
# Get data to test the model
_, _, x_test, y_test = get_data()

# Read the data of your TFLite model file
with open(tflite_model_file, 'rb') as f:
    tflite_model = f.read()
    
# Load TFLite model and allocate tensors
interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()

# Get input and output of model
input = interpreter.get_input_details()[0]
output = interpreter.get_output_details()[0]

# If you quantized your model to int8 only you have 
# convert your input data as int8 values
# x_test = x_test.astype(np.int8)

# Gather the results for the test data
predictions = []
for sample in x_test:
    # Set input data
    interpreter.set_tensor(input['index'], sample)
    # Run the model
    interpreter.invoke()
    # Get model output
    pred = interpreter.get_tensor(output['index'])
    predictions.append(pred.argmax())

model_acc = sum(1 for a,b in zip(predictions,y_test) if a == b) / len(predictions)

print('Model accuracy: {:.2f}%'.format(model_acc*100))
```

**Was muss beachtet werden**

Stellen Sie sicher, dass Ihr Gerät, wie der Raspberry Pi, über die erforderlichen Bibliotheken und Ressourcen verfügt, um das Modell auszuführen. Beachten Sie die Speicher- und Rechenanforderungen Ihres Modells und stellen Sie sicher, dass Ihr Gerät ausreichend leistungsfähig ist.

#### 2.4.6 **Monitoring (TinyMLOps)**

Die Überwachung der Modellleistung auf Edge-Geräten ist entscheidend, um sicherzustellen, dass das Modell auch nach Monaten noch wie gewünscht funktioniert. Hier sind einige Aspekte, die es zu berücksichtigen gilt:

Die Modellleistung kann im Laufe der Zeit abnehmen, insbesondere wenn das Modell auf sich ändernde Daten angewendet wird. Daher ist es wichtig, regelmäßige Überwachung und gegebenenfalls Aktualisierungen vorzunehmen, um sicherzustellen, dass das Modell genaue Vorhersagen trifft. Dies kann die Überwachung von Genauigkeitsmetriken, die Aktualisierung von Trainingsdaten und das Fine-Tuning des Modells umfassen. Das Monitoring ist ein kontinuierlicher Prozess.

**Metriken zur Überwachung**

Um die Leistung des Modells zu überwachen, sollten verschiedene Metriken und Kennzahlen regelmäßig ausgewertet werden. Hier sind einige wichtige Metriken:

1. **Genauigkeit (Accuracy):** Dies ist eine grundlegende Metrik, um zu überprüfen, wie oft das Modell korrekte Vorhersagen trifft.
2. **Präzision (Precision):** Die Präzision misst, wie viele der positiven Vorhersagen tatsächlich korrekt sind. Dies ist besonders wichtig, wenn Fehlalarme vermieden werden müssen.
3. **Recall:** Der Recall gibt an, wie viele der tatsächlichen positiven Fälle das Modell erkannt hat. Dies ist wichtig, um sicherzustellen, dass keine relevanten Ereignisse übersehen werden.
4. **F1-Score:** Der F1-Score ist das harmonische Mittel zwischen Präzision und Recall und bietet eine ausgewogene Bewertung der Modellleistung.
5. **AUC-ROC:** Der Flächeninhalt unter der Receiver Operating Characteristic (ROC)-Kurve ist eine Metrik für die Klassifikationsleistung, die besonders in binären Klassifikationsproblemen verwendet wird.

* **Aktualisierung von Trainingsdaten:** Um die Modellleistung aufrechtzuerhalten, kann es notwendig sein, die Trainingsdaten regelmäßig zu aktualisieren. Dies kann neue Datenpunkte, Aktualisierungen von Klassen oder veraltete Daten ersetzen. Die Daten sollten so repräsentativ wie möglich für die tatsächlichen Einsatzbedingungen sein.
* **Fine-Tuning des Modells:** Wenn die Modellleistung nachlässt, kann ein sogenanntes "Fine-Tuning" erforderlich sein. Dabei werden die Hyperparameter des Modells oder die Modellarchitektur angepasst, um die Leistung zu verbessern. Dies erfordert eine sorgfältige Analyse der Leistungsprobleme und gezielte Anpassungen.
* **Automatisierung der Überwachung:** Die regelmäßige Überwachung der Modellleistung kann zeitaufwändig sein. Daher kann es hilfreich sein, automatisierte Prozesse zur Modellüberwachung zu implementieren. Dies kann die Verwendung von Monitoring-Tools einschließen, die Warnungen ausgeben, wenn die Modellleistung unter bestimmte Schwellenwerte fällt.
* **Modellversionierung:** Es ist wichtig, eine klare Versionierung der Modelle beizubehalten, um Änderungen nachverfolgen zu können. Dies erleichtert es, zu vorherigen Modellversionen zurückzukehren, wenn Probleme mit neuen Versionen auftreten.

Die Modellüberwachung ist ein fortlaufender Prozess und stellt sicher, dass Ihr TinyML-Modell zuverlässig und präzise arbeitet, selbst nach längerem Einsatz auf Edge-Geräten. Es ist ein entscheidender Schritt, um die Vorteile von TinyML dauerhaft zu nutzen.
